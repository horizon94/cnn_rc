\subsubsection{Word Embeddings}
\label{sec:word_embeddings}

In \autoref{sec:neural_networks} the input layer of a neural network is modeled as neurons that output a real-value scalar or vector. In Natural Language Processing the input for classification problems are usually sentences consisting of words. It is not obvious how to convert discrete words in an input sentence to real numbers. A traditional approach is to convert each word into a probability of the word occuring given a previous sequence of $N-1$ words. This approach is called the \emph{N-gram model} and has been used for many years in NLP \citep[chapter 4]{jurafsky1994}. This approach suffers from many problems, however. The values of an N-gram lack semantic meaning outside of their context and suffer from the curse of dimensionality. A language vocabulary with 100,000 words and $N=5$ has a potential of $100,000^5 - 1 = 10^{25} - 1$ free parameters, which is huge while 5 words is a rather small window of understanding for a word. A better way of modeling words which also retains their semantic relationship is to use a \emph{distributed representation} for words, also known as \emph{word embeddings}. Each word is represented with a $m$-dimensional vector. A word embedding can be learned efficiently with a neural network on huge amounts of unlabeled data with modern computers \cite{bengio2003}. A particular neural network which is used to train word embeddings is called the \emph{skip-gram model}. The idea of skip-gram is similar to n-grams in that it tries to predict words in the context of a sentence. A skip-gram model used to produce word embeddings are also called \emph{word2vec} \cite{mikolov2013}. 

Word embeddings are commonly used by training a word embedding on huge amounts of unlabeled data and then subsequently using and fine-tuning the embedding on another NLP task such as RC. When used like this, word embeddings are an example of \emph{representation learning} and \emph{layer-wise pre-training} \citep[Chapter 15]{dl_book}. Another option is to randomly initialize the word embedding layer with random vectors and train them exclusively for a specific task. 